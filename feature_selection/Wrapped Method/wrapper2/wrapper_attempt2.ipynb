{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e0ff2ea-f405-4018-8912-81fc1efcadfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"padding-top: 10px;  padding-bottom: 10px;\">\n",
    "  <img src=\"https://insightfactoryai.sharepoint.com/:i:/r/sites/insightfactory.ai/Shared%20Documents/E.%20Marketing/Company%20Logos%20and%20Style%20Guide/PNG/insightfactory.ai%20logo%20multiline%20reversed.png\" alt='insightfactory.ai' width=150   style=\"display: block; margin: 0 auto\" /> \n",
    "</div>\n",
    "\n",
    "# Model Build\n",
    "\n",
    "***Summary of process:*** This notebook is used to Build the ML model from the engineered features. This results in table with performance of the model and version of the resulting model. It is also preferred to Match your resulting model version with the used pipeline version in the resultant. \n",
    "\n",
    "***Input Tables:***  \n",
    "- \n",
    "\n",
    "***Output Table:*** \n",
    "- model_config\n",
    "\n",
    "Note: This is just the overview of the process, For details please review the notebook thoroughly\n",
    "\n",
    "**Business Rules:** <br/>\n",
    "\\<Describe the Business Rules that are encapsulated in this Enrichment\\>\n",
    "\n",
    "**Dependencies:**<br/>\n",
    "-\n",
    "\n",
    "**Ownership:**<br/>\n",
    "\\<Indicate who owns this Enrichment ruleset\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2840a53d-87d0-49be-a155-08b66d113083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Modification Schedule\n",
    "\n",
    "| Date | Who | Description |\n",
    "| ---: | :--- | :--- |\n",
    "| 2025-09-12 | Yifan Gu  | Initial version. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb5b202-6673-4600-a843-bf53522d6f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Insight Factory Notebook Preparation\n",
    "\n",
    "**(Do not modify/delete the following cell)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ee19fa-d0a0-458b-95bb-385914de1b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (1.5.3)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (1.4.2)\nRequirement already satisfied: mlflow-skinny[databricks] in /databricks/python3/lib/python3.12/site-packages (2.19.0)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.12/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (2.2.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (0.30.0)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (3.1.37)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (1.27.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (1.27.0)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (24.1)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (4.24.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (6.0.1)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (2.32.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (0.5.1)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (12.17.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (2.18.2)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (1.34.69)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]) (1.34.69)\nRequirement already satisfied: azure-core>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]) (1.31.0)\nRequirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]) (12.23.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]) (4.11.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]) (0.6.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->mlflow-skinny[databricks]) (1.0.1)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->mlflow-skinny[databricks]) (0.10.2)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore->mlflow-skinny[databricks]) (1.26.16)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny[databricks]) (2.35.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny[databricks]) (4.0.11)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (2.20.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny[databricks]) (3.17.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny[databricks]) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny[databricks]) (0.48b0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny[databricks]) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny[databricks]) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny[databricks]) (2024.6.2)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]) (42.0.5)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny[databricks]) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny[databricks]) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (1.65.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]) (1.24.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny[databricks]) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny[databricks]) (4.9)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]) (1.16.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny[databricks]) (0.4.8)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: numpy>=1.17.0 in /databricks/python3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\nDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.6 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/3.6 MB\u001B[0m \u001B[31m537.1 kB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/3.6 MB\u001B[0m \u001B[31m537.1 kB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/3.6 MB\u001B[0m \u001B[31m522.7 kB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.2/3.6 MB\u001B[0m \u001B[31m708.3 kB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/3.6 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.9/3.6 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: lightgbm\nSuccessfully installed lightgbm-4.6.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn mlflow-skinny[databricks]\n",
    "%pip install -U lightgbm\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4d8dcb-6b3c-4a34-8ded-830745517bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/InsightFactory/Helpers/ML Build (Unity Catalog) Entry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89e4c6a-60cd-4ee4-b084-368f7fcd0260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, f1_score,precision_recall_curve\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af597e95-7397-415c-bbd4-4113e00c5dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Notebook Start\n",
    "\n",
    "### Input Parameters\n",
    "\n",
    "All Notebook Parameters (if any) are contained in the dictionary variable 'params'.  There are two ways to get the individual parameter from params, in both cases the parameter name is case-sensitive:\n",
    "\n",
    "  1) Use dot-notation - refer to the example below.\n",
    "\n",
    "      params = { \"Name\": \"Test\", \"Values\": { \"Title\": \"Results\", \"Results\": [ { \"Definition\": \"Core Sample\", \"Outcome\": \"Prospective\" }, { \"Definition\": \"Follow-up\", \"Outcome\": \"For review\" } ] } }\n",
    "\n",
    "      params.Name produces 'Test'<br/>\n",
    "      params.Values.Title produces 'Results'<br/>\n",
    "      params.Values.Results[0] produces { \"Definition\": \"Core Sample\", \"Outcome\": \"Prospective\" }<br/>\n",
    "      params.Values.Results[1].Definition produces 'Follow-up'\n",
    "\n",
    "  2) Use the search_dictionary function as follows:  var1 = search_dictionary(params, \"parameter-name\").  \n",
    "\n",
    "      There is an optional third parameter to this function: value_to_return_if_not_found -  this is the value to return if the particular parameter is not found in params.<br/>\n",
    "      **Note** that value_to_return_if_not_found can take on any type (string, int, boolean, struct, ..) e.g search_dictionary(params, \"IncorrectlyNamedParameter\", False) will return the boolean False if \"IncorrectlyNamedParameter\" is not found in params.\n",
    "\n",
    "**CAUTION:** There is another dictionary variable, 'config', that contains all of the configuration sent to this Notebook.  In most cases, you will have no use for 'config' but if you choose to use 'config' in this Notebook, note the following:\n",
    "- Access the individual parameters within config by using the search_dictionary function e.g. search_dictionary(config, \"ParameterName\").  Dot-notation access **does not apply** to 'config'.\n",
    "- Heed this **WARNING** - The individual parameter names within 'config' are subject to change outside of your control which may break your code.\n",
    "<br/><br/>\n",
    "\n",
    "### Enrichment Results\n",
    "\n",
    "Add the code you need to perform your enrichment/extract in cell(s) below until the 'Notebook End' cell.\n",
    "\n",
    "####Important: \n",
    "- Ensure that the result is stored in a PySpark dataframe as\n",
    "    - 'df_result' e.g. df_result = ...  containing Model Name, Model Version, Performance metrics, Pipeline version (for feature log) and model\n",
    "    - Ensure that your models are registered under ml_catalog. Always name your model as `f'{ml_catalog}.{delta_schema_name}.{model_name}'`\n",
    "\n",
    "This will result in model and model config Table in your shared ml_catalog for easy sharing and inference across environments.\n",
    "<br/><br/>\n",
    "\n",
    "### Running this Notebook directly in Databricks\n",
    "\n",
    "This Notebook can be run directly from your Databricks Workspace.  If the Notebook relies on Notebook Parameters, please read the following instructions:\n",
    "1) Add this line of code to a cell at the top of your Notebook and run that cell.<br/>\n",
    "   ```dbutils.widgets.text('ParametersJSON', '{ \"ModelName\":\"name\",\"ModelAlias\":\"alias\",\"ModelVersion\":\"version\",\"ModelSchema\":\"DatabaseName for Model\", \"NotebookParameters\": { \"param1\": \"value1\", \"param2\": \"value2\" } }')```\n",
    "2) This will add a Parameter to the Notebook.  Simply replace (or remove) the pre-canned parameters, 'param1', 'param2' and their values with your own.\n",
    "3) When you have finished running this Notebook directly in Databricks, comment out the line of code you added or delete the cell entirely.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08353efc-aa80-4768-925e-b1e69d3038be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = spark.table(\"`09ad024f-822f-48e4-9d9e-b5e03c1839a2`.feature_selection.total_training_table\")\n",
    "\n",
    "# keys = [\"Tc_BaseCode\", \"Tc_BaseCode_Mapped\", \"Tc_SectionBreakStartKM\", \"Tc_r_date\",\"p_key\"]\n",
    "# drop_cols = {\"Wagon_RecordingDate\",'Wagon_ICWVehicle'}\n",
    "\n",
    "# numeric_cols = [f.name for f in df.schema.fields\n",
    "#                 if isinstance(f.dataType, T.NumericType)\n",
    "#                 and f.name not in drop_cols and f.name not in keys]\n",
    "\n",
    "# other_keep = [c for c in df.columns if c not in keys and c not in drop_cols and c not in numeric_cols]\n",
    "# avg_aggs   = [F.avg(c).alias(c) for c in numeric_cols]\n",
    "# other_aggs = [F.first(c, ignorenulls=True).alias(c) for c in other_keep]\n",
    "\n",
    "# df_day = (df.groupBy(*keys).agg(*(avg_aggs + other_aggs))\n",
    "#             .withColumn(\"Tc_r_date\", F.to_date(\"Tc_r_date\")))\n",
    "\n",
    "# grp = [\"Tc_BaseCode\", \"Tc_BaseCode_Mapped\", \"Tc_SectionBreakStartKM\"]\n",
    "# w_order = Window.partitionBy(*grp).orderBy(F.col(\"Tc_r_date\").cast(\"timestamp\"))\n",
    "\n",
    "# def rolling_feats(df_in, cols, windows=[7,14,30]):\n",
    "#     df_out = df_in\n",
    "#     for c in cols:\n",
    "#         df_out = df_out.withColumn(f\"{c}_lag1\", F.lag(c, 1).over(w_order))\n",
    "\n",
    "#         for W in windows:\n",
    "#             w = w_order.rowsBetween(-W, -1)\n",
    "#             df_out = (df_out\n",
    "#                 .withColumn(f\"{c}_mean_{W}\", F.avg(F.col(f\"{c}_lag1\")).over(w))\n",
    "#                 .withColumn(f\"{c}_std_{W}\",  F.stddev(F.col(f\"{c}_lag1\")).over(w))\n",
    "#                 .withColumn(f\"{c}_max_{W}\",  F.max(F.col(f\"{c}_lag1\")).over(w))\n",
    "#                 .withColumn(f\"{c}_min_{W}\",  F.min(F.col(f\"{c}_lag1\")).over(w))\n",
    "#             )\n",
    "\n",
    "#         df_out = df_out.withColumn(\n",
    "#             f\"{c}_diff1\",\n",
    "#             F.col(f\"{c}_lag1\") - F.lag(f\"{c}_lag1\", 1).over(w_order)\n",
    "#         )\n",
    "\n",
    "#         if 7 in windows and 30 in windows:\n",
    "#             df_out = df_out.withColumn(\n",
    "#                 f\"{c}_sma7_div_sma30\",\n",
    "#                 F.when(F.col(f\"{c}_mean_30\").isNull() | (F.col(f\"{c}_mean_30\")==0), F.lit(None).cast(\"double\"))\n",
    "#                  .otherwise(F.col(f\"{c}_mean_7\")/F.col(f\"{c}_mean_30\"))\n",
    "#             )\n",
    "#     return df_out\n",
    "\n",
    "# sensor_cols = [\n",
    "#  'Wagon_Twist14m','Wagon_BounceFrt','Wagon_BounceRr','Wagon_BodyRockFrt','Wagon_BodyRockRr',\n",
    "#  'Wagon_LP1','Wagon_LP2','Wagon_LP3','Wagon_LP4','Wagon_Speed','Wagon_BrakeCylinder',\n",
    "#  'Wagon_IntrainForce','Wagon_Acc1','Wagon_Acc2','Wagon_Acc3','Wagon_Acc4','Wagon_Twist2m',\n",
    "#  'Wagon_Acc1_RMS','Wagon_Acc2_RMS','Wagon_Acc3_RMS','Wagon_Acc4_RMS','Wagon_Rail_Pro_L',\n",
    "#  'Wagon_Rail_Pro_R','Wagon_SND','Wagon_VACC','Wagon_VACC_L','Wagon_VACC_R','Wagon_Curvature',\n",
    "#  'Wagon_Track_Offset','Wagon_SND_L','Wagon_SND_R','w_row_count','Tng_Tonnage'\n",
    "# ]\n",
    "\n",
    "# df_time = rolling_feats(df_day, sensor_cols, windows=[7,14,30])\n",
    " \n",
    "# df_time = (df_time\n",
    "#     .withColumn(\"dow\", F.dayofweek(\"Tc_r_date\"))   # 1..7\n",
    "#     .withColumn(\"month\", F.month(\"Tc_r_date\"))\n",
    "#     .withColumn(\"sin_dow\",  F.sin(2*3.1415926*(F.col(\"dow\")-1)/7.0))\n",
    "#     .withColumn(\"cos_dow\",  F.cos(2*3.1415926*(F.col(\"dow\")-1)/7.0))\n",
    "#     .withColumn(\"sin_mon\",  F.sin(2*3.1415926*(F.col(\"month\")-1)/12.0))\n",
    "#     .withColumn(\"cos_mon\",  F.cos(2*3.1415926*(F.col(\"month\")-1)/12.0))\n",
    "# )\n",
    "\n",
    "# # w_hist = w_order.rowsBetween(Window.unboundedPreceding, -1)\n",
    "# # df_time = (df_time\n",
    "# #     .withColumn(\"last_break_date_hist\", F.last(\"Tc_break_date\", ignorenulls=True).over(w_hist))\n",
    "# #     # .withColumn(\"last_fail_date_hist\",  F.last(\"Tc_last_fail_if_available_otherwise_null\", ignorenulls=True).over(w_hist))\n",
    "# #     .withColumn(\"days_since_last_break\", F.when(F.col(\"last_break_date_hist\").isNull(), None)\n",
    "# #                 .otherwise(F.datediff(F.col(\"Tc_r_date\"), F.to_date(\"last_break_date_hist\"))))\n",
    "# #     #.withColumn(\"days_since_last_fail\", F.when(F.col(\"last_fail_date_hist\").isNull(), None)\n",
    "# #     #            .otherwise(F.datediff(F.col(\"Tc_r_date\"), F.to_date(\"last_fail_date_hist\"))))\n",
    "# # )\n",
    "\n",
    "# target_col = \"Tc_target\"\n",
    "# aux_cols = [\"Tc_BaseCode\",\"Tc_BaseCode_Mapped\",\"Tc_SectionBreakStartKM\",\"p_key\",\"Tc_r_date\",\"Tc_rul\",\"Tc_break_date\",                \"Tc_last_fail_if_available_otherwise_null\"]\n",
    "# #            \"Tc_last_fail_if_available_otherwise_null\",\"last_break_date_hist\"]\n",
    "\n",
    "# time_feat_cols = [c for c in df_time.columns\n",
    "#                   if any(s in c for s in [\"_mean_\",\"_std_\",\"_max_\",\"_min_\",\"_diff1\",\"_sma7_div_sma30\",\n",
    "#                                           \"sin_\",\"cos_\"])]\n",
    "\n",
    "# use_cols = aux_cols + sensor_cols + time_feat_cols + [target_col]\n",
    "# df_feat = df_time.select(*[c for c in use_cols if c in df_time.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac679e4-e8ee-4aa9-a83d-4b5b9345c1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (41867, 499) test: (3837, 499)\ntrain pos/neg: 4683 / 37184 => spw = 7.94\n"
     ]
    }
   ],
   "source": [
    "# pdf = df_feat.select(*use_cols).toPandas()\n",
    "# pdf[\"Tc_r_date\"] = pd.to_datetime(pdf[\"Tc_r_date\"])\n",
    "\n",
    "# pdf = pdf.sort_values(\n",
    "#     [\"Tc_r_date\",\"Tc_BaseCode\",\"Tc_BaseCode_Mapped\",\"Tc_SectionBreakStartKM\",\"p_key\"],\n",
    "#     kind=\"mergesort\"   \n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# import numpy as np\n",
    "# X_all = (pdf[sensor_cols + time_feat_cols]\n",
    "#          .apply(pd.to_numeric, errors=\"coerce\")\n",
    "#          .replace([np.inf, -np.inf], np.nan)\n",
    "#          .astype(\"double\"))\n",
    "# y_all = pdf[target_col].astype(int).to_numpy()\n",
    "\n",
    "# uniq_dates = np.sort(pdf[\"Tc_r_date\"].unique())\n",
    "# cutoff_date = uniq_dates[int(len(uniq_dates) * 0.8) - 1]   \n",
    "\n",
    "# train_mask = pdf[\"Tc_r_date\"] <= cutoff_date\n",
    "# test_mask  = pdf[\"Tc_r_date\"]  > cutoff_date\n",
    "\n",
    "# X_tr, y_tr = X_all[train_mask], y_all[train_mask]\n",
    "# X_te, y_te = X_all[test_mask],  y_all[test_mask]\n",
    "\n",
    "# print(\"train:\", X_tr.shape, \"test:\", X_te.shape)\n",
    "\n",
    "# pos = int(y_tr.sum())\n",
    "# neg = int(len(y_tr) - pos)\n",
    "# spw = max(1.0, neg / max(1, pos))\n",
    "# print(\"train pos/neg:\", pos, \"/\", neg, \"=> spw =\", round(spw,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dab8772-e8a9-4820-af66-1bc7d90812c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"`09ad024f-822f-48e4-9d9e-b5e03c1839a2`.feature_selection.preprocess_training_table\")\n",
    "\n",
    "# Grouping columns\n",
    "keys = [\"Tc_BaseCode\", \"Tc_BaseCode_Mapped\", \"Tc_SectionBreakStartKM\", \"Tc_r_date\"]\n",
    "\n",
    "# Exclude columns\n",
    "drop_cols = {\"Wagon_RecordingDate\",'Wagon_ICWVehicle'}\n",
    "\n",
    "# Averaging only numeric columns\n",
    "numeric_cols = [\n",
    "    f.name\n",
    "    for f in df.schema.fields\n",
    "    if isinstance(f.dataType, T.NumericType)          \n",
    "    and f.name not in drop_cols\n",
    "    and f.name not in keys\n",
    "]\n",
    "print(numeric_cols)\n",
    "\n",
    "\n",
    "# Constructing aggregate expressions\n",
    "avg_aggs = [F.avg(F.col(c)).alias(c) for c in numeric_cols]\n",
    "\n",
    "# Take the first non-empty column that is not numeric and not excluded\n",
    "other_keep = [\n",
    "    c for c in df.columns\n",
    "    if c not in keys and c not in drop_cols and c not in numeric_cols\n",
    "]\n",
    "other_aggs = [F.first(F.col(c), ignorenulls=True).alias(c) for c in other_keep]\n",
    "\n",
    "# Grouping + Aggregation\n",
    "df_merged = df.groupBy(*keys).agg(*(avg_aggs + other_aggs))\n",
    "\n",
    "# Fill all the averaged columns with 0 again\n",
    "df_merged = df_merged.fillna(0, subset=numeric_cols)\n",
    "\n",
    "df_merged.display()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b34cc4-a9ca-4afd-975a-bec8185e5924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_col = \"Tc_target\"\n",
    "aux_cols   = [\"Tc_BaseCode\", \"Tc_r_date\",'Tc_rul','Tc_break_date', 'Tc_last_fail_if_available_otherwise_null','Tc_BaseCode_Mapped', 'Tc_SectionBreakStartKM',] \n",
    "\n",
    "candidate_cols = [\n",
    "     'Wagon_Twist14m', 'Wagon_BounceFrt', 'Wagon_BounceRr', 'Wagon_BodyRockFrt', 'Wagon_BodyRockRr', 'Wagon_LP1', 'Wagon_LP2', 'Wagon_LP3', 'Wagon_LP4', 'Wagon_Speed', 'Wagon_BrakeCylinder', 'Wagon_IntrainForce', 'Wagon_Acc1', 'Wagon_Acc2', 'Wagon_Acc3', 'Wagon_Acc4', 'Wagon_Twist2m', 'Wagon_Acc1_RMS', 'Wagon_Acc2_RMS', 'Wagon_Acc3_RMS', 'Wagon_Acc4_RMS', 'Wagon_Rail_Pro_L', 'Wagon_Rail_Pro_R', 'Wagon_SND', 'Wagon_VACC', 'Wagon_VACC_L', 'Wagon_VACC_R', 'Wagon_Curvature', 'Wagon_Track_Offset', 'Wagon_SND_L', 'Wagon_SND_R', 'w_row_count', 'Tng_Tonnage'\n",
    "]\n",
    "\n",
    "use_cols = candidate_cols + [target_col] + aux_cols\n",
    "pdf = df_merged.select(*use_cols).toPandas()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_all = (pdf[candidate_cols]\n",
    "         .apply(pd.to_numeric, errors=\"coerce\")\n",
    "         .replace([np.inf, -np.inf], np.nan)\n",
    "         .fillna(0.0)\n",
    "         .astype(\"double\"))\n",
    "y_all = pdf[target_col].astype(int).values\n",
    "\n",
    "# Sorting by time segmentation\n",
    "pdf[\"_order\"] = pd.to_datetime(pdf[\"Tc_r_date\"])\n",
    "order = np.argsort(pdf[\"_order\"].values)\n",
    "X_all = X_all.iloc[order].reset_index(drop=True)\n",
    "y_all = y_all[order]\n",
    "pdf = pdf.iloc[order].reset_index(drop=True) \n",
    "# groups = pdf[\"Tc_BaseCode\"].values[order]   \n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(X_all) * split_ratio)\n",
    "train_mask = np.arange(len(X_all)) < split_idx\n",
    "test_mask  = ~train_mask\n",
    "\n",
    "X_tr, y_tr = X_all.loc[train_mask].reset_index(drop=True), y_all[train_mask]\n",
    "X_te, y_te = X_all.loc[test_mask].reset_index(drop=True), y_all[test_mask]\n",
    "\n",
    "print(\"train size:\", X_tr.shape, \"test size:\", X_te.shape)\n",
    "      \n",
    "pos = (y_tr == 1).sum()\n",
    "neg = (y_tr == 0).sum()\n",
    "spw = max(1.0, neg / max(1,pos))\n",
    "print(f\"train pos/neg: {pos}/{neg} = {spw:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb8f0af-f8df-4d3e-a1bc-5a2568d7a3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 478, number of negative: 6504\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114892 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 119964\n[LightGBM] [Info] Number of data points in the train set: 6982, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.068462 -> initscore=-2.610562\n[LightGBM] [Info] Start training from score -2.610562\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 1362, number of negative: 12597\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 120826\n[LightGBM] [Info] Number of data points in the train set: 13959, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097571 -> initscore=-2.224504\n[LightGBM] [Info] Start training from score -2.224504\n[LightGBM] [Info] Number of positive: 2092, number of negative: 18844\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288394 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 120976\n[LightGBM] [Info] Number of data points in the train set: 20936, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.099924 -> initscore=-2.198074\n[LightGBM] [Info] Start training from score -2.198074\n[LightGBM] [Info] Number of positive: 2813, number of negative: 25100\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.423214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 122928\n[LightGBM] [Info] Number of data points in the train set: 27913, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100777 -> initscore=-2.188616\n[LightGBM] [Info] Start training from score -2.188616\n[LightGBM] [Info] Number of positive: 3670, number of negative: 31220\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.731187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123232\n[LightGBM] [Info] Number of data points in the train set: 34890, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.105188 -> initscore=-2.140867\n[LightGBM] [Info] Start training from score -2.140867\nChosen threshold from CV (median): 0.124601\n[LightGBM] [Info] Number of positive: 4683, number of negative: 37184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.697361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123492\n[LightGBM] [Info] Number of data points in the train set: 41867, number of used features: 499\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.111854 -> initscore=-2.071940\n[LightGBM] [Info] Start training from score -2.071940\n\n=== TEST  ===\nPR-AUC: 0.30975165971153273\nF1    : 0.15980629539951574\nACC   : 0.72869429241595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_feats = [\n",
    "    'Wagon_Acc1', 'Wagon_Acc1_RMS', 'Wagon_Acc2', 'Wagon_Acc2_RMS',\n",
    "    'Wagon_Acc3', 'Wagon_Acc3_RMS', 'Wagon_Acc4', 'Wagon_Acc4_RMS'\n",
    "]\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "X_sel = X_tr[selected_feats]\n",
    "#X_sel = X_tr\n",
    "pr_scores, roc_scores, f1_scores, acc_scores = [], [], [], []\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=30,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=spw,\n",
    "    feature_pre_filter=False,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "ths = []\n",
    "for tr_idx, va_idx in tscv.split(X_sel):            \n",
    "    X_tr_thr = X_sel.iloc[tr_idx]\n",
    "    X_va_thr = X_sel.iloc[va_idx]\n",
    "\n",
    "    y_tr_thr = y_tr[tr_idx] \n",
    "    y_va_thr = y_tr[va_idx] \n",
    "\n",
    "    mdl = LGBMClassifier(**lgb.get_params())\n",
    "    mdl.fit(\n",
    "        X_tr_thr, y_tr_thr,\n",
    "        eval_set=[(X_va_thr, y_va_thr)],\n",
    "        eval_metric=\"aucpr\",\n",
    "        callbacks=[__import__(\"lightgbm\").early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    p_va = mdl.predict_proba(X_va_thr)[:, 1]\n",
    "    prec, reca, thr = precision_recall_curve(y_va_thr, p_va)  \n",
    "    f1 = 2 * prec * reca / (prec + reca + 1e-9)\n",
    "\n",
    "    ths.append(thr[np.nanargmax(f1[:-1])])   \n",
    "\n",
    "best_thr = float(np.median(ths))\n",
    "print(f\"Chosen threshold from CV (median): {best_thr:.6f}\")\n",
    "#best_thr = 0.999448\n",
    "\n",
    "# Retrain with the entire training set\n",
    "final_model = LGBMClassifier(**lgb.get_params())\n",
    "final_model.fit(X_sel, y_tr)\n",
    "\n",
    "# Do a final evaluation on the test set\n",
    "X_sel_te = X_te[selected_feats]\n",
    "#X_sel_te = X_te\n",
    "proba_te = final_model.predict_proba(X_sel_te)[:, 1]\n",
    "y_pred_te = (proba_te >= best_thr).astype(int)\n",
    "\n",
    "pr_auc = float(average_precision_score(y_te, proba_te))\n",
    "f1 = float(f1_score(y_te, y_pred_te))\n",
    "acc = float(accuracy_score(y_te, y_pred_te))\n",
    "\n",
    "print(\"\\n=== TEST  ===\")\n",
    "print(\"PR-AUC:\", pr_auc)\n",
    "print(\"F1    :\", f1)\n",
    "print(\"ACC   :\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9cae8a-1e2a-4e49-ab01-d28313a55c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold from CV (median): 0.999448\n预测为1的个数: 944\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Chosen threshold from CV (median): {best_thr:.6f}\")\n",
    "# n_pos_pred = int(np.sum(y_pred_te == 1))\n",
    "# print( n_pos_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df277f53-360c-413c-9393-7c6648b9f547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, time\n",
    "import mlflow, mlflow.lightgbm\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, accuracy_score\n",
    "from pyspark.sql import Row\n",
    "\n",
    "registered_name = f\"{ml_catalog}.{model_schema_name}.{model_name}\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_params({\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_child_samples\": 30,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"scale_pos_weight\": float(spw),\n",
    "        \"random_state\": 42,\n",
    "        \"selected_feat_cnt\": len(selected_feats),\n",
    "    })\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"PR_AUC\": pr_auc,\n",
    "        \"F1    \": f1,\n",
    "        \"ACC   \": acc\n",
    "    })\n",
    "\n",
    "    mlflow.log_dict(\n",
    "        {\"selected_feats\": selected_feats, \"threshold\": float(best_thr)},\n",
    "        artifact_file=\"rfe_lightgbm.json\"\n",
    "    )\n",
    "\n",
    "    ex_X = X_sel_te.head(5)\n",
    "    print(ex_X.columns)\n",
    "    sign = infer_signature(model_input=ex_X, model_output=final_model.predict_proba(ex_X)[:, 1])\n",
    "\n",
    "    mlflow.lightgbm.log_model(\n",
    "        final_model,\n",
    "        artifact_path=\"model\",                    \n",
    "        signature=sign,\n",
    "        input_example=ex_X.iloc[:1],\n",
    "        registered_model_name=registered_name      \n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"[MLflow] run_id = {run_id}\")\n",
    "\n",
    "client = MlflowClient()\n",
    "model_version = None\n",
    "for _ in range(10):\n",
    "    mvs = [m for m in client.search_model_versions(f\"name='{registered_name}'\") if m.run_id == run_id]\n",
    "    if mvs:\n",
    "        model_version = int(mvs[0].version)\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"[Registry] {registered_name} version = {model_version}\")\n",
    "\n",
    "df_result = spark.createDataFrame([\n",
    "    Row(\n",
    "        ModelName      = registered_name,                 \n",
    "        ModelVersion   = model_version,                \n",
    "        run_id          = run_id,\n",
    "        test_pr_auc     = pr_auc,\n",
    "        test_f1         = f1,\n",
    "        test_acc        = acc,\n",
    "        threshold       = float(best_thr),\n",
    "        selected_feats  = json.dumps(selected_feats),      \n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4a71fd-639a-4933-894b-a33b9a7d2226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################# Update your output data for the model configuration here #################\n",
    "# import pandas as pd\n",
    "# df_result=spark.createDataFrame(pd.DataFrame(\n",
    "#     data=[[]],\n",
    "#     columns=['ModelName','ModelVersion','ModelMetrics','PipelineVersion']\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a9ed090-f149-4a87-ad20-aa5f8bc46534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Notebook End\n",
    "\n",
    "**(Do not modify/delete the following cell)**\n",
    "\n",
    "####Important: \n",
    "1) Ensure that the result is stored in a PySpark dataframe as\n",
    "  - 'df_result' e.g. df_result = ...  containing Model Name, Model Version, Performance metrics, Pipeline version (for feature log)\n",
    "2) Ensure that your models are registered under ml_catalog. Always name your model as `f'{ml_catalog}.{delta_schema_name}.{model_name}'`\n",
    "\n",
    "This will result in model and model config Table in your shared ml_catalog for easy sharing and inference across environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "470c0e08-c08c-41c7-bbbd-b22e1832563c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/InsightFactory/Helpers/ML Build (Unity Catalog) Exit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b39bd7-3988-4f5a-8e3e-b34fe9746692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Testing or Debugging Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0edc4da-0d30-466d-9bb4-4747a5bd86f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wrapper_attempt2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}